# torch.nn

## 1.pytorch的损失函数

### 1.1 CrossEntropyLoss（交叉熵损失函数）

```
nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='elementwise_mean')
```

- weight(Tensor)- 为每个类别的 loss 设置权值，常用于类别不均衡问题。weight 必须是 float
  类型的 tensor，其长度要于类别C 一致，即每一个类别都要设置有weight。带weight 的计
  算公式：

- size_average(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False
  时，返回的各样本的 loss 之和。

- reduce(bool)- 返回值是否为标量，默认为 True

- ignore_index(int)- 忽略某一类别，不计算其 loss，其 loss 会为 0，并且，在采用size_average 时，不会计算那一类的 loss，除的时候的分母也不会统计那一类的样本。

- reduction 该参数在新版本中是为了取代size_average和reduce参数的。它共有三种选项 'elementwise_mean'，'sum'和'none'。'elementwise_mean'为默认情况，表明对N个样本的loss进行求平均之后返回(相当于reduce=True，size_average=True);'sum'指对n个样本的loss求和(相当于reduce=True，size_average=False);'none'表示直接返回n分样本的loss(相当于reduce=False)
  

## 2.pytorch的优化器

### 2.1 torch.optim.SGD

```
torch.optim.SGD(params, lr=<object object>, momentum=0, dampening=0, weight_decay=0, nesterov=False)
```

- params(iterable)- 参数组，优化器要管理的那部分参数。
- lr(float)- 初始学习率，可按需随着训练过程不断调整学习率